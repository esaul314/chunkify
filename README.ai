# README.ai

## Project Intent

This project aims to construct a robust, reusable, standalone Python library for chunking large **PDF and EPUB** documents, preparing them for training and further interactions within a local LLM environment. The implementation emphasizes declarative and functional programming principles, with clear adherence to Unix philosophy—creating simple, focused modules communicating through structured interfaces.

## Project State

*   The pipeline is successfully refactored to a **two-pass (Structural -> Semantic) architecture**, improving modularity and the quality of the output.
*   The **Structural Pass** is implemented, using **PyMuPDF** for advanced layout-aware PDF parsing and a refined BeautifulSoup strategy for EPUBs.
*   The **Semantic Pass** is implemented using Haystack's sentence-aware chunker to produce coherent text chunks.
*   **Traceable Metadata** is generated for each chunk, linking it back to its source file, page/location, and structural type (e.g., heading, paragraph).
*   The output format is a clean **JSONL**, with a **`--no-metadata` flag** for users who only require plain text chunks.
*   A declarative, functional approach to text cleaning is now implemented in the parsing module, ensuring consistent and robust whitespace handling.
*   The `_apply.sh` script is a robust testing tool for validating both PDF and EPUB processing.

## Desired End-State

*   A highly reliable chunking library that understands the semantic structure of various document formats.
*   A clear, multi-stage pipeline that separates structural analysis from semantic chunking.
*   A simple, clear CLI-based workflow facilitating rapid iteration and testing via shell scripts.

## Core Architecture: A Two-Pass Approach

The library is designed around a two-pass philosophy to maximize the semantic integrity of the final text chunks.

### 1. The Structural Pass
**Intention: Extract text while respecting and capturing the document's intrinsic structural logic.**

This pass is handled by the `parsing.py` module. It analyzes the source file and converts it into an intermediate representation: a list of structured text blocks (e.g., `[{'type': 'heading', 'text': '...'}, {'type': 'paragraph', 'text': '...'}]`).

-   **PDF Parsing**: Uses **`PyMuPDF` (`fitz`)** for its ability to provide detailed layout information. A heuristic based on font flags (e.g., bold) is used to distinguish headings from regular paragraphs.
-   **EPUB Parsing**: Uses `EbookLib` and `BeautifulSoup` to parse HTML content, identifying paragraphs and headings based on their block-level tags (`<p>`, `<h1>`, etc.).

### 2. The Semantic Pass
**Intention: Refine the structurally extracted text blocks into semantically cohesive chunks with traceable metadata.**

This pass is handled by the `splitter.py` and `utils.py` modules.

-   **Semantic Chunking (`splitter.py`)**: Takes all the structured blocks, combines their text into a single document, and uses **Haystack's `PreProcessor`** configured to be sentence-aware (`split_by="word"`, `split_respect_sentence_boundary=True`). This ensures that chunks do not end in the middle of a sentence, which is critical for preserving meaning for LLM training.
-   **Metadata Mapping (`utils.py`)**: This is the crucial final step. It takes the chunks produced by Haystack and, using a character-offset map, links each chunk back to the original structured block it started in. This allows it to stitch the correct source metadata (file, page, location, block type) to the final output chunk.

## Project Architecture

The library is organized into clearly defined modules and scripts:

```
pdf_chunker/
├── pdf_chunker/
│   ├── core.py          # Orchestrates the two-pass pipeline
│   ├── parsing.py       # Structural Pass: Extracts structured text blocks
│   ├── splitter.py      # Semantic Pass: Chunks combined text
│   └── utils.py         # Cleans text and maps final metadata
├── scripts/
│   └── chunk_pdf.py     # Command-line interface for the pipeline
...
```

### File Purposes and Evolution

*   **core.py**: Orchestrates the pipeline, passing data from the structural pass to the semantic pass.
*   **parsing.py**: **(Structural Pass)** Handles layout-aware text extraction and cleaning for both PDF (PyMuPDF) and EPUB (BeautifulSoup). It now contains pure, declarative functions for whitespace management.
*   **splitter.py**: **(Semantic Pass)** Implements sentence-aware chunking on the combined text from the structural pass.
*   **utils.py**: Contains helpers for **mapping metadata** from the original structured blocks onto the final chunks. The cleaning function has been removed from this module.
*   **chunk\_pdf.py**: CLI script that now includes a **`--no-metadata` flag** for flexible output.

## Usage and Testing

### Command-Line Usage
To process a document, run the following command from the project root:
```shell
# With metadata (default)
./pdf-env/bin/python -m scripts.chunk_pdf path/to/your/document.pdf > output.jsonl

# Without metadata
./pdf-env/bin/python -m scripts.chunk_pdf --no-metadata path/to/your/document.pdf > output_text_only.jsonl
```

### Validation with `_apply.sh`
The `_apply.sh` script is the primary tool for testing functionality.

-   **Test PDF**: `./_apply.sh pdf`
-   **Test EPUB**: `./_apply.sh epub`

## Coding Principles & Style Guide

1.  **Declarative & Functional Paradigm**: Implement pure functions wherever practical. Clearly define inputs and outputs without side effects.
2.  **Unix Philosophy**: "Do one thing and do it well." Keep modules small, focused, and composable.
3.  **Clear Interfaces & Data Flow**: The new two-pass architecture exemplifies this, with a clear, structured data format passing between the stages.
4.  **Testing**: The `_apply.sh` script is used at each checkpoint to validate functionality and correctness.
5.  **Iterative Refinement**: The evolution of the pipeline through several major refactors to arrive at this robust solution demonstrates this principle.
6.  **Metadata & Context Preservation**: The pipeline now excels at this, providing traceable and useful metadata for each chunk.

## Recommended Checkpoint Procedure

*   At every iteration:
    1.  Verify and validate outputs via the `_apply.sh` script for all supported formats.
    2.  Confirm stable functional behavior after changes.
    3.  Ensure seamless integration of new functionality with existing modules.

## Constraints & Preferences

*   Follow pure, explicit, and clean coding practices inspired by Bruce Eckel, Tom Gilb, and Bob Martin.
*   Prioritize simplicity, clarity, modularity, and explicitness.

---

This document provides guidance for effectively achieving the outlined objectives through structured checkpoints, clear testing protocols, and disciplined coding practices.
