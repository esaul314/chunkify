# README.ai

## Project Intent

This project aims to construct a robust, reusable, standalone Python library for chunking large **PDF and EPUB** documents, preparing them for training and further interactions within a local LLM environment. The implementation emphasizes declarative and functional programming principles, with clear adherence to Unix philosophy—creating simple, focused modules communicating through structured interfaces.

## Project State: Enhanced for RAG with Robust Processing Pipeline


The project has undergone significant evolution to produce RAG-supportive, context-aware metadata with enterprise-grade reliability and strict quality controls. The system now uses a **simplified PyMuPDF4LLM integration** that combines the best of both traditional and modern extraction approaches.
    


*   **Simplified PyMuPDF4LLM Integration**: The pipeline uses **traditional font-based extraction for all structural analysis** (headings, block boundaries, page metadata) while applying **PyMuPDF4LLM's superior text cleaning** for improved ligature handling, word joining, and whitespace normalization. This approach maintains proven reliability while leveraging PyMuPDF4LLM's evolving capabilities with reduced complexity.
    
*   **Three-Pass Architecture**: The pipeline follows a **three-pass (Structural → Semantic → AI Enrichment) architecture**, adding intelligent metadata generation with external configuration support.

*   **Enhanced PDF Extraction**: Implements a **three-tier fallback strategy** (PyMuPDF → pdftotext → pdfminer.six) with quality assessment and automatic fallback logic to handle problematic PDFs, now enhanced with optional PyMuPDF4LLM text cleaning.
    
*   **Page and Spine Exclusion Support**: Flexible filtering functionality allows excluding specific pages or ranges for PDFs (e.g., "1,3,5-10,15-20") and spine items for EPUBs to skip tables of contents, bibliographies, and other non-content sections.
*   **EPUB Spine Discovery**: Interactive spine listing with content sampling allows users to preview EPUB structure before processing, showing spine indices, filenames, and content previews (first 80 characters) for informed exclusion decisions.
*   **Modernized Core**: The entire processing backend has been upgraded to the modern **`haystack-ai`** framework, resolving critical dependency conflicts (`Pydantic V1 vs V2`) and ensuring future compatibility.
*   **Strict Semantic Chunking**: Implements comprehensive chunk size validation with force-splitting logic to prevent oversized chunks (8k character limit per chunk, emergency truncation at 25k characters).
*   **External Tag Configuration**: Uses **YAML-based tag vocabularies** for domain-specific AI enrichment, supporting generic tags plus specialized domains (philosophy, psychology, technical, project management).
*   **Modular Code Organization**: Refactored into focused modules (`text_cleaning.py`, `heading_detection.py`, `extraction_fallbacks.py`, `page_utils.py`, `epub_parsing.py`) following Unix philosophy principles.
*   **Modular Test Architecture**: Comprehensive test suite refactored from monolithic script into focused, reusable test modules with shared utilities (`tests/utils/common.sh`) and lightweight orchestration (`tests/run_all_tests.sh`).
*   **Enhanced Metadata Generation Pipeline**: Implements granular diagnostic logging throughout the chunk processing pipeline with detailed process tracing to prevent silent chunk loss and ensure all valid chunks are properly processed or logged with drop reasons.
*   **Robust Error Handling**: Added comprehensive error handling and validation in the metadata generation pipeline to prevent chunks from being silently dropped during AI enrichment or final formatting phases.


*   **Rich Metadata Generation**: The pipeline generates comprehensive RAG-supportive metadata for each chunk, including:
    *   **Language Detection**: Using `langdetect` on a per-block basis.
    *   **Readability Scores**: Calculated using `textstat` to provide Flesch-Kincaid grade levels.
    *   **AI-Powered Classification**: LLM-based utterance type classification (e.g., `definition`, `instruction`, `example`).
    *   **Domain-Specific Tags**: Configurable tag vocabularies for specialized content domains.
*   **Robust Text Extraction**: EPUB parsing handles inline formatting (`<i>`, `<b>`, etc.) and strips invisible Unicode characters like the Byte Order Mark (`U+FEFF`), with hyphenated word rejoining for clean text processing.
*   **Enhanced Text Processing**: Advanced Unicode ligature normalization, hyphen handling for line breaks, and block-level merging for improved text continuity.

## Desired End-State

*   A highly reliable chunking library that understands the semantic structure of various document formats.
*   A clear, multi-stage pipeline that separates structural analysis from semantic chunking and AI enrichment.
*   A simple, clear CLI-based workflow facilitating rapid iteration and testing via shell scripts.
*   Strict quality controls ensuring no oversized chunks or problematic JSONL output.
*   Configurable AI enrichment supporting diverse content domains through external tag vocabularies.

## Core Architecture: Enhanced Three-Pass Approach

The library is designed around an enhanced three-pass philosophy to maximize semantic integrity, reliability, and usefulness of the final text chunks. The system now incorporates a **simplified PyMuPDF4LLM integration** that provides superior text quality while maintaining proven structural analysis.

### 1. The Structural Pass (Enhanced with Simplified PyMuPDF4LLM Integration)
**Intention: Extract clean, structured text from the source document with maximum reliability and superior text quality.**

This pass is handled by the `parsing.py` module with support from specialized modules. It analyzes the source file and converts it into an intermediate representation: a list of structured text blocks with enhanced text quality.

#### Simplified PyMuPDF4LLM Integration
*   **Traditional Structural Analysis**: **`PyMuPDF` (`fitz`)** handles all structural analysis including font-based heading detection, block boundaries, and page metadata  
*   **PyMuPDF4LLM Text Enhancement**: **`PyMuPDF4LLM`** provides superior text cleaning for ligature translation, word joining, and whitespace normalization  
*   **Automatic Fallback**: Falls back to traditional three-tier system (PyMuPDF → pdftotext → pdfminer.six) if PyMuPDF4LLM text cleaning fails  
*   **Reduced Complexity**: Eliminates complex hybrid extraction logic while maintaining the benefits of both approaches  
*   **Future-Ready**: Positions the system to leverage PyMuPDF4LLM's evolving capabilities as they improve  

#### Robust Text Processing
*   **EPUB Parsing**: Uses `EbookLib` and `BeautifulSoup` with functional approach (`_get_element_text_content`) for inline formatting handling  
*   **Enhanced Text Cleaning**: PyMuPDF4LLM-enhanced `_clean_paragraph` function with superior ligature handling and word joining, plus traditional BOM removal and hyphenated word break fixes  
*   **Heading Detection**: Traditional font flag analysis with defensive error handling and fallback heuristics for unusual PDF structures  
*   **Language Detection**: `langdetect` applied to each text block with graceful error handling  

### 2. The Semantic Pass (Strict Size Validation)
**Intention: Refine the structurally extracted text into semantically cohesive chunks with strict size controls.**

This pass is handled by the `splitter.py` module with comprehensive validation and force-splitting logic.

*   **Technology**: Modern **`haystack-ai`** framework with `DocumentSplitter`  
*   **Conservative Configuration**:  
    *   Target: 500-800 words per chunk (configurable)  
    *   Overlap: 50 words (25% maximum)  
    *   Sentence boundary respect enabled  
*   **Strict Size Validation**:  
    *   8,000 character limit per chunk  
    *   10,000 character limit for JSONL lines  
    *   Emergency truncation at 25,000 characters  
*   **Multi-Stage Force-Splitting**:  
    *   Paragraph-based splitting for oversized chunks  
    *   Sentence-based splitting for large paragraphs  
    *   Character-based splitting as last resort  
    *   Sentence boundary preservation where possible  

#### Semantic Chunker Parameters
The semantic chunker behavior is controlled by key parameters in `pdf_chunker/splitter.py`:

*   **`split_length`** (default: 500-800): Maximum number of words per chunk  
*   **`split_overlap`** (default: 50): Number of words to overlap between adjacent chunks  
*   **`respect_sentence_boundary`** (default: True): Ensures chunks never end mid-sentence  
*   **`max_chars_per_chunk`** (default: 8000): Strict character limit with force-splitting  

### 3. The AI Enrichment Pass (External Configuration Support)
**Intention: Add semantic understanding and domain-specific tags to each chunk for advanced RAG applications.**

This pass is handled by `ai_enrichment.py` with external YAML configuration support.

#### External Tag Configuration System
*   **Configuration Structure**: YAML files in `config/tags/` directory  
*   **Generic Tags**: Base categories (content_type, complexity, actionability) in `config/tags/generic.yaml`  
*   **Domain-Specific Tags**: Specialized vocabularies for:  
    *   Philosophy (`config/tags/philosophy.yaml`): Schools, concepts, thinkers, methods  
    *   Psychology (`config/tags/psychology.yaml`): Schools, concepts, research methods, applications  
    *   Technical (`config/tags/technical.yaml`): Languages, technologies, concepts, levels  
    *   Project Management (`config/tags/project_management.yaml`): Methodologies, phases, concepts, tools  
*   **Tag Loading**: Automatic merging of multiple YAML files with duplicate removal  
*   **Tag Validation**: LLM responses validated against available tag vocabulary  

#### AI Processing Features
*   **Utterance Type Classification**: LLM-based classification (definition, explanation, instruction, etc.)  
*   **Domain-Specific Tagging**: 2-4 relevant tags selected from configured vocabularies  
*   **Flexible LLM Provider**: Uses `litellm` for 100+ LLM provider support  
*   **Parallel Processing**: `ThreadPoolExecutor` for efficient LLM call handling  
*   **Structured Responses**: JSON-like format parsing with error handling  

## Project Architecture (Modular Organization)

The codebase has been refactored into focused, single-responsibility modules following Unix philosophy principles:

```
pdf_chunker/
├── .env                           # API keys for AI services
├── config/
│   └── tags/                      # External tag configuration
│       ├── generic.yaml           # Base tag categories
│       ├── philosophy.yaml        # Philosophy domain tags
│       ├── psychology.yaml        # Psychology domain tags
│       ├── technical.yaml         # Technical domain tags
│       └── project_management.yaml # PM domain tags
├── pdf_chunker/
│   ├── core.py                    # Orchestrates the three-pass pipeline
│   ├── parsing.py                 # Structural Pass: Main extraction orchestration
│   ├── text_cleaning.py           # Text cleaning and preprocessing utilities
│   ├── heading_detection.py       # Heading detection heuristics and fallbacks
│   ├── extraction_fallbacks.py    # Fallback extraction strategies (pdftotext, pdfminer)
│   ├── page_utils.py              # Page range parsing and validation utilities
│   ├── epub_parsing.py            # EPUB extraction with spine exclusion support
│   ├── splitter.py                # Semantic Pass: Chunks text with strict validation
│   ├── utils.py                   # Maps metadata and orchestrates AI enrichment
│   └── ai_enrichment.py           # AI Pass: Classification and tagging with external configs
├── scripts/
│   ├── chunk_pdf.py               # Command-line interface for the pipeline
│   ├── validate_chunks.sh         # Structural validation and quality checks
│   └── detect_duplicates.py       # Duplicate detection and overlap analysis
├── tests/                         # Modular test architecture
│   ├── utils/
│   │   └── common.sh              # Shared test utilities and formatting functions
│   ├── pdf_extraction_test.py     # Focused PDF extraction testing with fallback validation
│   ├── ai_enrichment_test.py      # AI enrichment and tag configuration testing
│   ├── semantic_chunking_test.py  # Chunk size validation and module integration testing
│   ├── page_exclusion_test.py     # PDF page exclusion functionality testing
│   ├── epub_spine_test.py         # EPUB spine exclusion functionality testing
│   └── run_all_tests.sh           # Test orchestrator that coordinates all test modules
└── _apply.sh                      # Lightweight dependency installer and test launcher
```
    
### Modular Design Benefits

**Focused Responsibilities:**
- `text_cleaning.py`: Pure text preprocessing functions (BOM removal, hyphenated word rejoining, whitespace normalization)
- `heading_detection.py`: Fallback heuristics for heading detection when font analysis fails
- `extraction_fallbacks.py`: Alternative PDF extraction strategies with quality assessment
- `page_utils.py`: Page range parsing and validation with flexible syntax support

**Clear Interfaces:**
Each module exposes simple, testable functions with explicit inputs and outputs, making the codebase easier to understand, test, and maintain.

**Unix Philosophy Adherence:**
Small, composable modules that do one thing well and communicate through structured data interfaces.

## Usage and Testing

### One-Command Comprehensive Workflow

### Modular Test Infrastructure

The project provides a streamlined validation workflow accessible via a single command:

```bash
# Run the complete pipeline with comprehensive validation
./_apply.sh
```

This executes the following lightweight sequence:
1. **Dependency Installation**: Installs Python dependencies and system tools (poppler-utils)
2. **Test Orchestration**: Delegates to `tests/run_all_tests.sh` for comprehensive validation
3. **Modular Test Execution**: Runs focused test modules for each feature area

The test suite follows the same modular architecture principles as the main codebase, with focused modules that can be run independently or as part of the comprehensive test orchestration.

### Modular Test Architecture

The test suite has been refactored into focused, reusable modules following the same Unix philosophy principles as the main codebase:

The test suite has been refactored into focused, reusable modules:
**Test Orchestrator (`tests/run_all_tests.sh`):**
- Coordinates execution of all test modules
- Aggregates results and provides overall success/failure status
- Uses shared utilities for consistent reporting

**Individual Test Modules:**
- `tests/pdf_extraction_test.py`: Tests three-tier fallback strategy with quality metrics
- `tests/ai_enrichment_test.py`: Validates tag configuration loading and LLM integration
- `tests/semantic_chunking_test.py`: Verifies strict size limits and force-splitting logic
- `tests/page_exclusion_test.py`: Tests PDF page filtering functionality with edge cases
- `tests/epub_spine_test.py`: Tests EPUB spine exclusion with comprehensive validation

**Shared Utilities (`tests/utils/common.sh`):**
- File discovery functions for consistent test file handling
- Result formatting utilities for standardized output
- Error handling patterns for robust test execution

**Enhanced Testing Coverage:**
**Enhanced Diagnostic Testing:**
- **Metadata Generation Pipeline Testing**: Comprehensive validation of chunk processing with detailed process tracing
- **Chunk Loss Prevention**: Granular logging to ensure all valid chunks are either processed or logged with drop reasons
- **Page Exclusion Validation**: Tests both range parsing accuracy and metadata generation pipeline compliance
- **EPUB Spine Discovery Testing**: Validates content sampling and preview generation functionality

### EPUB Spine Discovery

The EPUB chunker includes interactive spine discovery to help users identify content structure before processing.

**Basic Spine Listing:**

```bash
# List all spine items with indices and filenames
python scripts/chunk_pdf.py book.epub --list-spines
```

**Enhanced Content Sampling:**
The spine discovery feature provides content previews (first 80 characters) to help identify sections:

```
EPUB Spine Structure (15 items):
  1. cover.xhtml - Cover Page
  2. titlepage.xhtml - Title: The Complete Guide to...
  3. toc.xhtml - Table of Contents Chapter 1...
  4. chapter01.xhtml - Chapter 1: Introduction In this...
  5. chapter02.xhtml - Chapter 2: Getting Started The...
```

**Workflow for EPUB Processing:**
1. **Discovery**: Use `--list-spines` to examine EPUB structure and content
2. **Exclusion Planning**: Identify spine items to exclude (ToC, cover, bibliography)
3. **Processing**: Run with `--exclude-pages` using spine indices

**Content Sampling Features:**
- **Text Extraction**: Uses existing EPUB parsing utilities for clean content extraction
- **Truncation**: Automatically truncates previews to 80 characters with ellipsis
- **Error Handling**: Gracefully handles spine items with no readable content
- **Functional Design**: Pure functions with clear data transformations

### Enhanced Diagnostic Capabilities

The system includes comprehensive diagnostic logging and process tracing to ensure reliable chunk processing and metadata generation.

**Metadata Generation Pipeline Diagnostics:**
- **Process Entry/Exit Tracking**: Detailed logging of chunk processing entry, validation, and exit points
- **Source Block Mapping Validation**: Traces how chunks are mapped back to original source blocks for metadata
- **AI Enrichment Monitoring**: Logs before and after AI enrichment calls to identify processing failures
- **Chunk Loss Prevention**: Ensures all chunks either produce output or are logged with specific drop reasons
- **Character Map Validation**: Verifies that excluded pages aren't included in character mapping for metadata generation

**Page Exclusion Diagnostics:**
- **Range Parsing Validation**: Detailed logging of how page range specifications are parsed and expanded
- **Exclusion Application Tracking**: Traces how exclusions are applied during PDF extraction and metadata generation
- **Boundary Calculation Verification**: Validates that range boundaries are calculated correctly (e.g., "1-7" excludes exactly pages 1-7)
- **Pipeline Compliance**: Ensures exclusions are respected throughout the entire processing pipeline

**Process Tracing Features:**
- **Chunk-Level Tracking**: Individual chunk processing with entry/exit logging and error capture
- **Pipeline Stage Validation**: Verification that chunks flow correctly through structural → semantic → AI enrichment phases
- **Error Isolation**: Granular error handling to prevent silent failures and provide actionable debugging information
- **Performance Monitoring**: Tracking of processing times and resource usage for optimization

### Page Exclusion Functionality

The PDF chunker supports flexible page exclusion to skip non-content pages like tables of contents, bibliographies, and front/back matter.

**Supported Syntax:**
- Individual pages: "1,3,5"
- Page ranges: "1-5" or "10-15"
- Mixed format: "1,3,5-10,15-20"

**Command-Line Usage:**
```bash
# Exclude first page and bibliography (pages 45-50)
python scripts/chunk_pdf.py document.pdf --exclude-pages "1,45-50"

# Exclude table of contents and index
python scripts/chunk_pdf.py book.pdf --exclude-pages "1-3,200-205"
```

**Features:**
- **Validation**: Automatically validates page ranges against document length
- **Error Handling**: Graceful handling of invalid syntax or out-of-bounds pages
- **Fallback Support**: Works consistently across all three extraction methods (PyMuPDF, pdftotext, pdfminer.six)
- **Logging**: Clear feedback about which pages were excluded during processing

**Edge Case Handling:**
- Pages beyond document bounds are automatically filtered out with warnings
- Invalid syntax (e.g., "1-2-3") is rejected with clear error messages
- Empty or malformed specifications are safely ignored
- Attempts to exclude all pages are prevented with warnings

### Setup Requirements

1. **Install dependencies**: `./pdf-env/bin/pip install -r requirements.txt`
2. **Create environment file**: Add your API key to `.env`:
   ```
   OPENAI_API_KEY=your_api_key_here
   ```
3. **Configure tag vocabularies**: Customize YAML files in `config/tags/` for your domains
4. **Enable debug mode** (optional): `export PDF_CHUNKER_DEBUG=1`

### Tag Configuration Examples

#### Generic Tags (`config/tags/generic.yaml`)
```yaml
content_type:
  - definition
  - explanation
  - example
  - methodology

complexity:
  - beginner
  - intermediate
  - advanced

actionability:
  - actionable
  - theoretical
  - reference
```

#### Domain-Specific Tags (`config/tags/philosophy.yaml`)
```yaml
philosophical_schools:
  - phenomenology
  - existentialism
  - analytic_philosophy

concepts:
  - consciousness
  - being
  - ethics
  - metaphysics
```

## Validation and Quality Assurance


The project includes comprehensive validation tooling with a modular test architecture to ensure chunk quality and detect issues:

### Modular Test Infrastructure

**Focused Test Modules**: Each test module concentrates on a specific feature area with consistent patterns:
- **Isolated Testing**: Individual modules can be run independently for targeted validation
- **Consistent Reporting**: Standardized success/failure reporting across all test modules
- **Reusable Utilities**: Shared functions in `tests/utils/common.sh` eliminate code duplication
- **Comprehensive Coverage**: Each module tests both unit-level functionality and end-to-end pipeline behavior

**Test Orchestration**: The `tests/run_all_tests.sh` script provides:
- **Sequential Execution**: Runs all test modules in logical order
- **Aggregated Results**: Combines individual test results into overall success/failure status
- **Early Termination**: Stops on critical failures while allowing non-critical warnings
- **Lightweight Coordination**: Minimal orchestration logic that delegates to focused test modules

**Enhanced Test Coverage**:
- **PDF Extraction Testing**: Validates three-tier fallback strategy with quality assessment metrics
- **AI Enrichment Testing**: Tests tag configuration loading, LLM integration, and tag validation
- **Semantic Chunking Testing**: Verifies strict size limits, force-splitting logic, and JSONL compatibility
- **Page Exclusion Testing**: Comprehensive validation of PDF page filtering with edge case handling
- **EPUB Spine Testing**: Tests spine exclusion functionality with range parsing and validation
- **Module Integration Testing**: Validates refactored module imports and cross-module functionality

## JSONL Output Schema
    

**Important Note**: This documentation reflects the actual JSON structure produced by the code. It's crucial that documentation matches implementation to prevent confusion for downstream consumers.

Each line in the output JSONL file contains a chunk with the following structure:

```json
{
  "text": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. This approach allows systems to automatically learn and improve performance on a specific task through experience.",
  "metadata": {
    "source": "sample_book.pdf",
    "chunk_id": "sample_book_p12_c003",
    "page": 12,
    "location": null,
    "block_type": "paragraph",
    "language": "en",
    "readability": {
      "flesch_kincaid_grade": 12.3,
      "difficulty": "college_level"
    },
    "utterance_type": {
      "classification": "explanation",
      "tags": ["technical", "artificial_intelligence", "definition"]
    },
    "importance": "medium"
  }
}
```

**Key Schema Details:**
- **`text`**: The actual chunk content (guaranteed ≤ 8,000 characters with force-splitting)
- **`metadata.source`**: Original filename
- **`metadata.chunk_id`**: Unique identifier following pattern `{filename}_p{page}_c{chunk_index}`
- **`metadata.page`**: Source page number (may have accuracy issues with repeating content)
- **`metadata.location`**: For EPUBs, contains internal file path; null for PDFs
- **`metadata.block_type`**: Either "heading" or "paragraph" based on font analysis or fallback heuristics
- **`metadata.language`**: ISO language code from `langdetect` (defaults to "un" for unknown)
- **`metadata.readability`**: Simplified readability metrics with `flesch_kincaid_grade` and `difficulty` level
- **`metadata.utterance_type`**: Object containing both `classification` (LLM-determined content type) and `tags` (array of domain-specific tags from YAML configurations)
- **`metadata.importance`**: Currently defaults to "medium" (placeholder for future enhancement)

**Configuration Impact on Output:**
- With AI enrichment disabled: `utterance_type.classification` will be "disabled" and `tags` array will be empty
- Tag vocabularies are loaded from external YAML files in `config/tags/` and validated against LLM responses
- Each JSONL line is validated to be ≤ 10,000 characters to prevent oversized output

### Structural Validator (`scripts/validate_chunks.sh`)

**Purpose**: Validates the structural integrity and quality of generated chunks.

**Key Features:**
- **Exit Codes**: Returns `0` for success, `1` for validation failures, `2` for file not found
- **Flags**: `-v` for verbose output, `-q` for quiet mode
- **Validation Checks**:
  - Empty text detection (chunks with missing or whitespace-only content)
  - Mid-sentence start detection (chunks beginning with lowercase letters)
  - Overlong chunk detection (chunks >8000 characters)
  - JSON structure validation

**Sample Output:**
```
✓ Validating output_chunks_pdf.jsonl...
✓ Structural validation complete:
  Total chunks: 486
  Empty text: 0
  Mid-sentence starts: 2
  Overlong chunks: 0

✅ Validation PASSED - no critical anomalies detected
```

### Duplicate Detector (`scripts/detect_duplicates.py`)

**Purpose**: Analyzes chunks for both intentional semantic overlap and problematic duplication.

**Detection Methodology:**
- **Sliding Window Analysis**: Uses configurable window size (default: 50 words) with minimum overlap threshold (default: 10 words)
- **Boundary Analysis**: Examines overlap between consecutive chunks
- **Distinction**: Separates intentional edge overlap from problematic full-paragraph repetition

### Comprehensive Testing Infrastructure (`_apply.sh`)

**Purpose**: Provides end-to-end validation of the entire pipeline with detailed reporting.

**Testing Coverage:**
- **PDF Extraction Quality**: Tests three-tier fallback strategy with quality metrics
- **Chunk Size Validation**: Ensures strict adherence to size limits (8k chars, 10k JSONL lines)
- **AI Enrichment**: Validates tag loading, LLM integration, and tag validation
- **Module Integration**: Tests refactored module imports and functionality
- **Error Handling**: Validates KeyError fixes and graceful degradation
- **Page/Spine Exclusion**: Comprehensive testing of filtering functionality for both PDFs and EPUBs
- **JSONL Compatibility**: Ensures output meets line length requirements
- **Process Tracing**: Detailed logging validation to prevent silent chunk drops
- **Page Exclusion**: Comprehensive testing of page filtering functionality

**Page Exclusion Testing:**
The testing infrastructure includes comprehensive validation of the page exclusion feature:
- **Baseline vs. Excluded**: Compares extraction with and without page exclusions
- **Single Page Exclusion**: Tests excluding individual pages (e.g., page 1)
- **Multiple Page Exclusion**: Tests excluding multiple pages (e.g., "1,3")
- **Range Exclusion**: Tests excluding page ranges (e.g., "1-2")
- **Edge Cases**: Tests invalid page numbers, out-of-bounds ranges, and malformed syntax
- **Full Pipeline**: Validates page exclusion works through the complete processing pipeline
- **Fallback Methods**: Ensures all three extraction methods respect page exclusions

**Test Execution Flow:**
1. Dependency installation and tool verification
2. PDF extraction testing with quality assessment
3. AI enrichment testing with tag configuration validation
4. KeyError fix validation for unusual PDF structures
5. Chunk size validation with force-splitting verification
6. Module integration testing for refactored components
7. Page exclusion functionality testing with comprehensive edge cases
8. Final JSONL output validation for size compliance

## Known Limitations and Recent Fixes

### Current Limitations

1. **Page Number Mapping**: Multi-page PDFs with repeating content may have incorrect page-to-chunk associations  
2. **Debug Output**: Page-level debugging requires manual environment variable setting (`PDF_CHUNKER_DEBUG=1`)

### Recent Fixes and Enhancements

1. **Page Exclusion Functionality**: Added comprehensive page filtering support with flexible range syntax ("1,3,5-10,15-20") for excluding non-content pages
2. **Modular Code Organization**: Refactored into focused modules (`text_cleaning.py`, `heading_detection.py`, `extraction_fallbacks.py`, `page_utils.py`) for better maintainability
3. **Enhanced Chunk Size Validation**: Implemented strict size limits with multi-stage force-splitting logic to prevent oversized JSONL lines
4. **KeyError Handling**: Added defensive checks for unusual PDF block structures with fallback heading detection
5. **Hyphenated Word Rejoining**: Fixed line-break hyphens (e.g., "itera-tion" → "iteration") in text cleaning
6. **Dependency Conflicts**: Resolved pdfminer.six version conflicts with pdfplumber
7. **Command-Line Interface**: added `--exclude-pages` parameter with comprehensive validation and error handling
8. **Testing Infrastructure**: Enhanced `_apply.sh` with comprehensive validation covering all functionality including page exclusion edge cases

### Quality Assurance Metrics

- **Chunk Size Limits**: 8,000 characters per chunk, 10,000 per JSONL line
- **Quality Thresholds**: Extraction quality score ≥ 0.7 for primary method acceptance
- **Fallback Triggers**: Automatic fallback when space density < 0.05 or avg line length > 1000
- **Tag Validation**: All LLM-generated tags validated against configured vocabularies

## Coding Principles & Style Guide

### Declarative & Functional Paradigm
*   **Pure Functions**: Implement pure functions wherever practical with clearly defined inputs and outputs without side effects. This is the foundation of the codebase architecture.
*   **Functional Techniques**: Lean heavily towards functional programming concepts and techniques. Prioritize Python comprehensions, generator expressions, and functional composition over imperative loops and stateful operations.
*   **Data Transformation**: Achieve data transformation through comprehensions and generator expressions, as seen in the EPUB parsing logic in `parsing.py` and tag configuration loading in `ai_enrichment.py`.
*   **Immutable Data Flow**: Prefer immutable data structures and avoid stateful operations wherever possible. Pass data through transformation pipelines rather than modifying objects in place.
*   **Functional Composition**: Build complex operations by composing simple, pure functions. Each function should do one thing well and be easily testable in isolation.
*   **Declarative Style**: Write code that expresses *what* should happen rather than *how* it should happen. Use descriptive function names and clear data transformations.

### Unix Philosophy
*   **Single Responsibility**: "Do one thing and do it well." Keep modules small, focused, and composable. The refactored modules (`text_cleaning.py`, `heading_detection.py`, `extraction_fallbacks.py`) exemplify this principle.
*   **Composable Interfaces**: Design modules to interact through simple textual or structured JSON outputs.
*   **Pipeline Architecture**: The three-pass architecture demonstrates clear separation of concerns with well-defined interfaces.
*   **Modular Testing**: The refactored test architecture follows the same principles—focused test modules that do one thing well and compose into a comprehensive test suite.

### Clear Interfaces & Data Flow
*   **Explicit Boundaries**: Maintain explicit input/output boundaries for predictable, testable behavior.
*   **Parameter Passing**: Avoid stateful or implicit behaviors; prefer explicit parameter passing.
*   **Structured Data**: Use consistent data structures (dictionaries with standardized keys) throughout the pipeline.
*   **Error Handling**: Implement graceful error handling with clear error messages and fallback strategies.
*   **Functional Error Handling**: Use functional patterns for error handling where appropriate (e.g., returning Result-like structures rather than raising exceptions for expected failures).

### Code Structure and Organization
*   **Small, Focused Files**: Break down large methods into smaller, more focused methods. Extract reusable code into separate methods or classes.
*   **Design Patterns**: Implement design patterns where appropriate (e.g., Strategy pattern for extraction fallbacks).
*   **Module Separation**: Separate concerns into focused modules:
    *   `text_cleaning.py`: Text preprocessing and cleaning utilities
    *   `heading_detection.py`: Heading detection heuristics and fallbacks
    *   `extraction_fallbacks.py`: Alternative extraction strategies
    *   `ai_enrichment.py`: LLM integration and tag management

### Code Quality Standards
*   **DRY Principle**: Remove redundant code and unnecessary comments. Apply the "Don't Repeat Yourself" principle consistently.
*   **Meaningful Names**: Use meaningful and consistent variable and method names that clearly indicate purpose and scope.
*   **Defensive Programming**: Add defensive checks for unexpected data structures (as seen in the KeyError fixes).
*   **Comprehensive Logging**: Implement proper logging and debugging mechanisms for troubleshooting and monitoring.

### Testing and Validation
*   **Iterative Refinement**: The evolution from a two-pass to a three-pass system with external configuration demonstrates this principle.
*   **Quality Metrics**: Implement comprehensive quality metrics and validation at each stage of the pipeline.
*   **Automated Testing**: Use the `_apply.sh` script for automated end-to-end validation and regression testing.
*   **Edge Case Handling**: Test and handle edge cases (unusual PDF structures, oversized chunks, malformed content).

## Constraints & Preferences


### Programming Paradigm Adherence
*   **Functional Style**: Follow pure functional, explicit, and clean coding practices inspired by Bruce Eckel, Tom Gilb, and Bob Martin. Prioritize functional programming techniques over object-oriented or imperative approaches.
*   **Simplicity First**: Prioritize simplicity, clarity, modularity, and explicitness over clever or complex solutions. Choose the most straightforward functional approach.
*   **Code Smell Elimination**: Identify and remove code smells, such as duplicate code, long methods, complex conditional statements, and stateful operations.
*   **SOLID Principles**: Apply SOLID principles to enhance the overall design and maintainability of the code, with particular emphasis on Single Responsibility and Dependency Inversion.
*   **Functional Composition**: Build complex functionality by composing simple, pure functions rather than creating large, monolithic functions or classes.
*   **Performance Awareness**: Consider time and space complexity, using generators, lazy evaluation, and parallelism where appropriate.

### Performance and Optimization
*   **Efficiency**: Optimize performance where applicable, considering time and space complexity while maintaining functional purity.
*   **Memory Management**: Be mindful of memory usage, especially when processing large documents. Use generators and lazy evaluation where appropriate.
*   **Parallel Processing**: Use parallel processing for I/O-bound operations (LLM calls, file processing) while maintaining functional principles.
*   **Lazy Evaluation**: Use generator expressions and lazy evaluation extensively to improve memory efficiency and performance.

### Error Handling and Robustness
*   **Graceful Degradation**: Implement fallback strategies that allow the system to continue functioning when components fail.
*   **Comprehensive Validation**: Validate data at each stage of the pipeline with clear error messages.
*   **Defensive Programming**: Add defensive checks for unexpected inputs and edge cases using functional validation patterns.
*   **Logging Strategy**: Implement structured logging for debugging and monitoring without cluttering the output.
*   **Functional Error Patterns**: Use functional error handling patterns (Option/Maybe types, Result types) where appropriate instead of exception-heavy code.

### Documentation and Maintainability
*   **Self-Documenting Code**: Write code that is self-explanatory through good naming and clear functional structure.
*   **Minimal Comments**: Lean towards no comments as much as possible. Include comments only when necessary for understanding complex functional transformations.
*   **Configuration Documentation**: Provide clear examples and documentation for external configuration files.
*   **Testing Documentation**: Document testing procedures and validation criteria, emphasizing the modular test architecture.
*   **Functional Documentation**: Document the data flow and transformations clearly, showing how data moves through the functional pipeline.

### External Dependencies and Configuration
*   **Minimal Dependencies**: Use well-established, maintained libraries with permissive licenses that support functional programming patterns.
*   **External Configuration**: Prefer external configuration files (YAML) over hard-coded values for domain-specific settings.
*   **Environment Management**: Use environment variables for sensitive configuration (API keys).
*   **Dependency Management**: Maintain clean, conflict-free dependency specifications.
*   **Modular Architecture**: Design systems as collections of small, focused modules that can be easily tested, understood, and maintained independently.

### Test Architecture and Quality Assurance
*   **Modular Testing**: Follow the established pattern of focused test modules with shared utilities rather than monolithic test scripts.
*   **Functional Testing**: Write tests that validate functional transformations and data flow rather than implementation details.
*   **Test Composition**: Build comprehensive test suites by composing focused test modules, following the same principles as the main codebase.
*   **Shared Utilities**: Extract common test patterns into reusable utilities to eliminate duplication and ensure consistency.
*   **Declarative Test Logic**: Write tests that clearly express what is being validated rather than how the validation is performed.
*   **Modular Test Infrastructure**: Tests follow lightweight orchestration using shell scripts and shared utilities.