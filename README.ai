# README.ai

## Project Intent

This project aims to construct a robust, reusable, standalone Python library for chunking large **PDF and EPUB** documents, preparing them for training and further interactions within a local LLM environment. The implementation emphasizes declarative and functional programming principles, with clear adherence to Unix philosophy—creating simple, focused modules communicating through structured interfaces.

## Project State

*   The pipeline has been refactored to a **two-pass (Structural -> Semantic) architecture**, improving modularity and the quality of the output.
*   The **Structural Pass** is implemented, using **PyMuPDF** for advanced layout-aware PDF parsing and a refined BeautifulSoup strategy for EPUBs.
*   The **Semantic Pass** is implemented using Haystack's sentence-aware chunker to produce coherent text chunks.
*   The output format is a clean **JSONL**, suitable for LoRA training pipelines.
*   The `_apply.sh` script is a robust testing tool for validating both PDF and EPUB processing.

## Desired End-State

*   A highly reliable chunking library that understands the semantic structure of various document formats.
*   A clear, multi-stage pipeline that separates structural analysis from semantic chunking.
*   A simple, clear CLI-based workflow facilitating rapid iteration and testing via shell scripts.

## Core Architecture: A Two-Pass Approach

The library is designed around a two-pass philosophy to maximize the semantic integrity of the final text chunks.

### 1. The Structural Pass
**Intention: Extract text while respecting and capturing the document's intrinsic structural logic.**

This pass is handled by the `parsing.py` module. It analyzes the source file and converts it into an intermediate representation: a list of structured text blocks (e.g., `[{'type': 'heading', 'text': '...'}, {'type': 'paragraph', 'text': '...'}]`).

-   **PDF Parsing**: Uses **`PyMuPDF` (`fitz`)** for its ability to provide detailed layout information. A heuristic based on font flags (e.g., bold) is used to distinguish headings from regular paragraphs.
-   **EPUB Parsing**: Uses `EbookLib` and `BeautifulSoup` to parse HTML content, identifying paragraphs and headings based on their tags (`<p>`, `<h1>`, etc.).

### 2. The Semantic Pass
**Intention: Refine the structurally extracted text blocks into semantically cohesive chunks.**

This pass is handled by the `splitter.py` module. It takes the cleaned, structured blocks, combines them into a single text document, and then uses a sophisticated chunker to create the final output.

-   **Semantic Chunking**: Uses **Haystack's `PreProcessor`** configured to be sentence-aware (`split_by="word"`, `split_respect_sentence_boundary=True`). This ensures that chunks do not end in the middle of a sentence, which is critical for preserving meaning for LLM training.

## Project Architecture

The library is organized into clearly defined modules and scripts:

```
pdf_chunker/
├── pdf_chunker/
│   ├── core.py          # Orchestrates the two-pass pipeline
│   ├── parsing.py       # Structural Pass: Extracts structured text blocks from source files
│   ├── splitter.py      # Semantic Pass: Chunks text using a sentence-aware strategy
│   └── utils.py         # Cleans text within the structured blocks
├── scripts/
│   └── chunk_pdf.py     # Command-line interface for triggering chunking operations
...
```

### File Purposes and Evolution

*   **core.py**: Orchestrates the pipeline, passing data from the structural pass (parsing) to cleaning and then to the semantic pass (splitting).
*   **parsing.py**: **(Structural Pass)** Handles layout-aware text extraction for PDF (PyMuPDF) and EPUB (BeautifulSoup) into a common structured format.
*   **splitter.py**: **(Semantic Pass)** Implements sentence-aware chunking using Haystack's PreProcessor.
*   **utils.py**: Contains utility functions, primarily for cleaning the text content within the structured blocks generated by the parser.
*   **chunk\_pdf.py**: CLI script ensuring usability and integration with shell-based workflows.

## Usage and Testing

### Command-Line Usage
To process a document, run the following command from the project root:
```shell
./pdf-env/bin/python -m scripts.chunk_pdf path/to/your/document.pdf > output.jsonl
```
This command works for both `.pdf` and `.epub` files. You can also provide optional arguments like `--chunk_size` and `--overlap`.

### Validation with `_apply.sh`
The `_apply.sh` script is the primary tool for testing and validating functionality.

-   **Test the default PDF (`sample-local-pdf.pdf`):**
    ```shell
    ./_apply.sh pdf
    ```
-   **Test the default EPUB (`accessible_epub_3.epub`):**
    ```shell
    ./_apply.sh epub
    ```

## Coding Principles & Style Guide

1.  **Declarative & Functional Paradigm**: Implement pure functions wherever practical. Clearly define inputs and outputs without side effects.
2.  **Unix Philosophy**: "Do one thing and do it well." Keep modules small, focused, and composable.
3.  **Clear Interfaces & Data Flow**: The new two-pass architecture exemplifies this, with a clear, structured data format passing between the stages.
4.  **Testing**: The `_apply.sh` script is used at each checkpoint to validate functionality and correctness.
5.  **Iterative Refinement**: The evolution of the pipeline from a simple script to a two-pass architecture demonstrates this principle in action.
6.  **Metadata & Context Preservation**: The structural pass in `parsing.py` now extracts useful metadata (source page, location, block type) which can be used in the future.

## Recommended Checkpoint Procedure

*   At every iteration:
    1.  Verify and validate outputs via the `_apply.sh` script for all supported formats.
    2.  Confirm stable functional behavior after changes.
    3.  Ensure seamless integration of new functionality with existing modules.

## Constraints & Preferences

*   Follow pure, explicit, and clean coding practices inspired by Bruce Eckel, Tom Gilb, and Bob Martin.
*   Prioritize simplicity, clarity, modularity, and explicitness.

---

This document provides guidance for effectively achieving the outlined objectives through structured checkpoints, clear testing protocols, and disciplined coding practices.
