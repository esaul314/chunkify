# README.ai

## Project Intent

This project aims to construct a robust, reusable, standalone Python library for chunking large **PDF and EPUB** documents, preparing them for training and further interactions within a local LLM environment. The implementation emphasizes declarative and functional programming principles, with clear adherence to Unix philosophy—creating simple, focused modules communicating through structured interfaces.

## Project State: Enhanced for RAG with Robust Processing Pipeline

The project has undergone significant evolution to produce RAG-supportive, context-aware metadata with enterprise-grade reliability and strict quality controls.

> **⚠️ Known Issue:** There are edge cases relating to page numbering in multi-page PDFs with repeating content across pages, where chunk-to-source block mapping may assign incorrect page numbers. This can lead to chunks appearing in a non-intuitive order in the output JSONL. A potential solution would involve implementing a more sophisticated page-aware heuristic in the `_find_source_block` function in `utils.py`.

*   **Three-Pass Architecture**: The pipeline follows a **three-pass (Structural → Semantic → AI Enrichment) architecture**, adding intelligent metadata generation with external configuration support.
*   **Enhanced PDF Extraction**: Implements a **three-tier fallback strategy** (PyMuPDF → pdftotext → pdfminer.six) with quality assessment and automatic fallback logic to handle problematic PDFs.
*   **Modernized Core**: The entire processing backend has been upgraded to the modern **`haystack-ai`** framework, resolving critical dependency conflicts (`Pydantic V1 vs V2`) and ensuring future compatibility.
*   **Strict Semantic Chunking**: Implements comprehensive chunk size validation with force-splitting logic to prevent oversized chunks (8k character limit per chunk, emergency truncation at 25k characters).
*   **External Tag Configuration**: Uses **YAML-based tag vocabularies** for domain-specific AI enrichment, supporting generic tags plus specialized domains (philosophy, psychology, technical, project management).
*   **Modular Code Organization**: Refactored into focused modules (`text_cleaning.py`, `heading_detection.py`, `extraction_fallbacks.py`) following Unix philosophy principles.
*   **Rich Metadata Generation**: The pipeline generates comprehensive RAG-supportive metadata for each chunk, including:
    *   **Language Detection**: Using `langdetect` on a per-block basis.
    *   **Readability Scores**: Calculated using `textstat` to provide Flesch-Kincaid grade levels.
    *   **AI-Powered Classification**: LLM-based utterance type classification (e.g., `definition`, `instruction`, `example`).
    *   **Domain-Specific Tags**: Configurable tag vocabularies for specialized content domains.
*   **Robust Text Extraction**: EPUB parsing handles inline formatting (`<i>`, `<b>`, etc.) and strips invisible Unicode characters like the Byte Order Mark (`U+FEFF`), with hyphenated word rejoining for clean text processing.
*   **Comprehensive Testing**: Integrated testing infrastructure in `_apply.sh` validates extraction quality, chunk sizes, AI enrichment, and module functionality.

## Desired End-State

*   A highly reliable chunking library that understands the semantic structure of various document formats.
*   A clear, multi-stage pipeline that separates structural analysis from semantic chunking and AI enrichment.
*   A simple, clear CLI-based workflow facilitating rapid iteration and testing via shell scripts.
*   Strict quality controls ensuring no oversized chunks or problematic JSONL output.
*   Configurable AI enrichment supporting diverse content domains through external tag vocabularies.

## Core Architecture: Enhanced Three-Pass Approach

The library is designed around an enhanced three-pass philosophy to maximize semantic integrity, reliability, and usefulness of the final text chunks.

### 1. The Structural Pass (Enhanced with Fallback Strategy)
**Intention: Extract clean, structured text from the source document with maximum reliability.**

This pass is handled by the `parsing.py` module with support from specialized modules. It analyzes the source file and converts it into an intermediate representation: a list of structured text blocks.

#### Enhanced PDF Extraction with Three-Tier Fallback
*   **Primary Method**: **`PyMuPDF` (`fitz`)** for layout-aware parsing with quality assessment
*   **Fallback Method 1**: **`pdftotext -layout`** for layout preservation when PyMuPDF quality is poor
*   **Fallback Method 2**: **`pdfminer.six`** with tunable LAParams as final option
*   **Quality Assessment**: Automatic evaluation of extraction quality using average line length, space density, and combined quality scores
*   **Automatic Fallback Logic**: Tries methods in order based on quality thresholds (quality score < 0.7 triggers fallback)

#### Robust Text Processing
*   **EPUB Parsing**: Uses `EbookLib` and `BeautifulSoup` with functional approach (`_get_element_text_content`) for inline formatting handling
*   **Text Cleaning**: Enhanced `_clean_paragraph` function strips Unicode BOM (`\uFEFF`) and fixes hyphenated word breaks (e.g., "itera-tion" → "iteration")
*   **Heading Detection**: Font flag analysis with defensive error handling and fallback heuristics for unusual PDF structures
*   **Language Detection**: `langdetect` applied to each text block with graceful error handling

### 2. The Semantic Pass (Strict Size Validation)
**Intention: Refine the structurally extracted text into semantically cohesive chunks with strict size controls.**

This pass is handled by the `splitter.py` module with comprehensive validation and force-splitting logic.

*   **Technology**: Modern **`haystack-ai`** framework with `DocumentSplitter`
*   **Conservative Configuration**: 
    *   Target: 500-800 words per chunk (configurable)
    *   Overlap: 50 words (25% maximum)
    *   Sentence boundary respect enabled
*   **Strict Size Validation**: 
    *   8,000 character limit per chunk
    *   10,000 character limit for JSONL lines
    *   Emergency truncation at 25,000 characters
*   **Multi-Stage Force-Splitting**:
    *   Paragraph-based splitting for oversized chunks
    *   Sentence-based splitting for large paragraphs
    *   Character-based splitting as last resort
    *   Sentence boundary preservation where possible

#### Semantic Chunker Parameters

The semantic chunker behavior is controlled by key parameters in `pdf_chunker/splitter.py`:

*   **`split_length`** (default: 500-800): Maximum number of words per chunk
*   **`split_overlap`** (default: 50): Number of words to overlap between adjacent chunks
*   **`respect_sentence_boundary`** (default: True): Ensures chunks never end mid-sentence
*   **`max_chars_per_chunk`** (default: 8000): Strict character limit with force-splitting

### 3. The AI Enrichment Pass (External Configuration Support)
**Intention: Add semantic understanding and domain-specific tags to each chunk for advanced RAG applications.**

This pass is handled by `ai_enrichment.py` with external YAML configuration support.

#### External Tag Configuration System
*   **Configuration Structure**: YAML files in `config/tags/` directory
*   **Generic Tags**: Base categories (content_type, complexity, actionability) in `config/tags/generic.yaml`
*   **Domain-Specific Tags**: Specialized vocabularies for:
    *   Philosophy (`config/tags/philosophy.yaml`): Schools, concepts, thinkers, methods
    *   Psychology (`config/tags/psychology.yaml`): Schools, concepts, research methods, applications
    *   Technical (`config/tags/technical.yaml`): Languages, technologies, concepts, levels
    *   Project Management (`config/tags/project_management.yaml`): Methodologies, phases, concepts, tools
*   **Tag Loading**: Automatic merging of multiple YAML files with duplicate removal
*   **Tag Validation**: LLM responses validated against available tag vocabulary

#### AI Processing Features
*   **Utterance Type Classification**: LLM-based classification (definition, explanation, instruction, etc.)
*   **Domain-Specific Tagging**: 2-4 relevant tags selected from configured vocabularies
*   **Flexible LLM Provider**: Uses `litellm` for 100+ LLM provider support
*   **Parallel Processing**: `ThreadPoolExecutor` for efficient LLM call handling
*   **Structured Responses**: JSON-like format parsing with error handling

## Project Architecture (Modular Organization)
```
pdf_chunker/
├── .env                           # API keys for AI services
├── config/
│   └── tags/                      # External tag configuration
│       ├── generic.yaml           # Base tag categories
│       ├── philosophy.yaml        # Philosophy domain tags
│       ├── psychology.yaml        # Psychology domain tags
│       ├── technical.yaml         # Technical domain tags
│       └── project_management.yaml # PM domain tags
├── pdf_chunker/
│   ├── core.py                    # Orchestrates the three-pass pipeline
│   ├── parsing.py                 # Structural Pass: Main extraction orchestration
│   ├── text_cleaning.py           # Text cleaning and preprocessing utilities
│   ├── heading_detection.py       # Heading detection heuristics and fallbacks
│   ├── extraction_fallbacks.py    # Fallback extraction strategies (pdftotext, pdfminer)
│   ├── splitter.py                # Semantic Pass: Chunks text with strict validation
│   ├── utils.py                   # Maps metadata and orchestrates AI enrichment
│   └── ai_enrichment.py           # AI Pass: Classification and tagging with external configs
├── scripts/
│   ├── chunk_pdf.py               # Command-line interface for the pipeline
│   ├── validate_chunks.sh         # Structural validation and quality checks
│   └── detect_duplicates.py       # Duplicate detection and overlap analysis
├── _e2e_check.sh                  # End-to-end test runner
└── _apply.sh                      # Comprehensive testing and validation workflow
```

## Usage and Testing

### One-Command Comprehensive Workflow

The project provides a streamlined validation workflow accessible via a single command:

```bash
# Run the complete pipeline with comprehensive validation
./_apply.sh
```

This executes the following comprehensive sequence:
1. **Dependency Installation**: Installs Python dependencies and system tools (poppler-utils)
2. **PDF Extraction Testing**: Tests three-tier fallback strategy with quality metrics
3. **AI Enrichment Testing**: Validates tag configuration loading and LLM integration
4. **KeyError Fix Validation**: Ensures robust handling of unusual PDF structures
5. **Chunk Size Validation**: Verifies strict size limits and force-splitting logic
6. **Module Integration Testing**: Validates refactored module imports and functionality
7. **JSONL Output Validation**: Ensures no oversized lines in final output

### Individual Command Usage

For more granular control:

```bash
# Generate chunks from PDF
python scripts/chunk_pdf.py ./sample_book.pdf

# Validate generated chunks
./scripts/validate_chunks.sh output_chunks_pdf.jsonl

# Analyze duplicate patterns (detailed reporting)
python scripts/detect_duplicates.py output_chunks_pdf.jsonl
```

### Setup Requirements

1. **Install dependencies**: `./pdf-env/bin/pip install -r requirements.txt`
2. **Create environment file**: Add your API key to `.env`:
   ```
   OPENAI_API_KEY=your_api_key_here
   ```
3. **Configure tag vocabularies**: Customize YAML files in `config/tags/` for your domains
4. **Enable debug mode** (optional): `export PDF_CHUNKER_DEBUG=1`

### Tag Configuration Examples

#### Generic Tags (`config/tags/generic.yaml`)
```yaml
content_type:
  - definition
  - explanation
  - example
  - methodology

complexity:
  - beginner
  - intermediate
  - advanced

actionability:
  - actionable
  - theoretical
  - reference
```

#### Domain-Specific Tags (`config/tags/philosophy.yaml`)
```yaml
philosophical_schools:
  - phenomenology
  - existentialism
  - analytic_philosophy

concepts:
  - consciousness
  - being
  - ethics
  - metaphysics
```


## Validation and Quality Assurance

The project includes comprehensive validation tooling to ensure chunk quality and detect issues:

## JSONL Output Schema

**Important Note**: This documentation reflects the actual JSON structure produced by the code. It's crucial that documentation matches implementation to prevent confusion for downstream consumers.

Each line in the output JSONL file contains a chunk with the following structure:

```json
{
  "text": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. This approach allows systems to automatically learn and improve performance on a specific task through experience.",
  "metadata": {
    "source": "sample_book.pdf",
    "chunk_id": "sample_book_p12_c003",
    "page": 12,
    "location": null,
    "block_type": "paragraph",
    "language": "en",
    "readability": {
      "flesch_kincaid_grade": 12.3,
      "difficulty": "college_level"
    },
    "utterance_type": {
      "classification": "explanation",
      "tags": ["technical", "artificial_intelligence", "definition"]
    },
    "importance": "medium"
  }
}
```

**Key Schema Details:**

- **`text`**: The actual chunk content (guaranteed ≤ 8,000 characters with force-splitting)
- **`metadata.source`**: Original filename
- **`metadata.chunk_id`**: Unique identifier following pattern `{filename}_p{page}_c{chunk_index}`
- **`metadata.page`**: Source page number (may have accuracy issues with repeating content)
- **`metadata.location`**: For EPUBs, contains internal file path; null for PDFs
- **`metadata.block_type`**: Either "heading" or "paragraph" based on font analysis or fallback heuristics
- **`metadata.language`**: ISO language code from `langdetect` (defaults to "un" for unknown)
- **`metadata.readability`**: Simplified readability metrics with `flesch_kincaid_grade` and `difficulty` level
- **`metadata.utterance_type`**: Object containing both `classification` (LLM-determined content type) and `tags` (array of domain-specific tags from YAML configurations)
- **`metadata.importance`**: Currently defaults to "medium" (placeholder for future enhancement)

**Configuration Impact on Output:**

- With AI enrichment disabled: `utterance_type.classification` will be "disabled" and `tags` array will be empty
- Tag vocabularies are loaded from external YAML files in `config/tags/` and validated against LLM responses
- Each JSONL line is validated to be ≤ 10,000 characters to prevent oversized output

### Structural Validator (`scripts/validate_chunks.sh`)
    

**Purpose**: Validates the structural integrity and quality of generated chunks.

**Key Features:**
- **Exit Codes**: Returns `0` for success, `1` for validation failures, `2` for file not found
- **Flags**: `-v` for verbose output, `-q` for quiet mode
- **Validation Checks**: 
  - Empty text detection (chunks with missing or whitespace-only content)
  - Mid-sentence start detection (chunks beginning with lowercase letters)
  - Overlong chunk detection (chunks >8000 characters)
  - JSON structure validation

**Sample Output:**
```
✓ Validating output_chunks_pdf.jsonl...
✓ Structural validation complete:
  Total chunks: 486
  Empty text: 0
  Mid-sentence starts: 2
  Overlong chunks: 0

✅ Validation PASSED - no critical anomalies detected
```

### Duplicate Detector (`scripts/detect_duplicates.py`)

**Purpose**: Analyzes chunks for both intentional semantic overlap and problematic duplication.

**Detection Methodology:**
- **Sliding Window Analysis**: Uses configurable window size (default: 50 words) with minimum overlap threshold (default: 10 words)
- **Boundary Analysis**: Examines overlap between consecutive chunks
- **Distinction**: Separates intentional edge overlap from problematic full-paragraph repetition

### Comprehensive Testing Infrastructure (`_apply.sh`)

**Purpose**: Provides end-to-end validation of the entire pipeline with detailed reporting.

**Testing Coverage:**
- **PDF Extraction Quality**: Tests three-tier fallback strategy with quality metrics
- **Chunk Size Validation**: Ensures strict adherence to size limits (8k chars, 10k JSONL lines)
- **AI Enrichment**: Validates tag loading, LLM integration, and tag validation
- **Module Integration**: Tests refactored module imports and functionality
- **Error Handling**: Validates KeyError fixes and graceful degradation
- **JSONL Compatibility**: Ensures output meets line length requirements

## Known Limitations and Recent Fixes

### Current Limitations

1. **Page Number Mapping**: Multi-page PDFs with repeating content may have incorrect page-to-chunk associations  
2. **Debug Output**: Page-level debugging requires manual environment variable setting (`PDF_CHUNKER_DEBUG=1`)

### Recent Fixes and Enhancements

1. **KeyError Handling**: Added defensive checks for unusual PDF block structures with fallback heading detection
2. **Chunk Size Validation**: Implemented strict size limits with multi-stage force-splitting logic
3. **Hyphenated Word Rejoining**: Fixed line-break hyphens (e.g., "itera-tion" → "iteration")
4. **Dependency Conflicts**: Resolved pdfminer.six version conflicts with pdfplumber
5. **Modular Organization**: Refactored large files into focused modules for better maintainability

### Quality Assurance Metrics

- **Chunk Size Limits**: 8,000 characters per chunk, 10,000 per JSONL line
- **Quality Thresholds**: Extraction quality score ≥ 0.7 for primary method acceptance
- **Fallback Triggers**: Automatic fallback when space density < 0.05 or avg line length > 1000
- **Tag Validation**: All LLM-generated tags validated against configured vocabularies

## Coding Principles & Style Guide

### Declarative & Functional Paradigm
*   **Pure Functions**: Implement pure functions wherever practical with clearly defined inputs and outputs without side effects.
*   **Functional Techniques**: Lean towards using functional programming concepts and techniques. When appropriate, use Python comprehensions and generator expressions.
*   **Data Transformation**: Achieve data transformation through comprehensions and generator expressions, as seen in the EPUB parsing logic in `parsing.py` and tag configuration loading in `ai_enrichment.py`.
*   **Immutable Data Flow**: Prefer immutable data structures and avoid stateful operations where possible.

### Unix Philosophy
*   **Single Responsibility**: "Do one thing and do it well." Keep modules small, focused, and composable. The refactored modules (`text_cleaning.py`, `heading_detection.py`, `extraction_fallbacks.py`) exemplify this principle.
*   **Composable Interfaces**: Design modules to interact through simple textual or structured JSON outputs.
*   **Pipeline Architecture**: The three-pass architecture demonstrates clear separation of concerns with well-defined interfaces.

### Clear Interfaces & Data Flow
*   **Explicit Boundaries**: Maintain explicit input/output boundaries for predictable, testable behavior.
*   **Parameter Passing**: Avoid stateful or implicit behaviors; prefer explicit parameter passing.
*   **Structured Data**: Use consistent data structures (dictionaries with standardized keys) throughout the pipeline.
*   **Error Handling**: Implement graceful error handling with clear error messages and fallback strategies.

### Code Structure and Organization
*   **Small, Focused Files**: Break down large methods into smaller, more focused ones. Extract reusable code into separate methods or classes.
*   **Design Patterns**: Implement design patterns where appropriate (e.g., Strategy pattern for extraction fallbacks).
*   **Module Separation**: Separate concerns into focused modules:
    *   `text_cleaning.py`: Text preprocessing and cleaning utilities
    *   `heading_detection.py`: Heading detection heuristics and fallbacks
    *   `extraction_fallbacks.py`: Alternative extraction strategies
    *   `ai_enrichment.py`: LLM integration and tag management

### Code Quality Standards
*   **DRY Principle**: Remove redundant code and unnecessary comments. Apply the "Don't Repeat Yourself" principle consistently.
*   **Meaningful Names**: Use meaningful and consistent variable and method names that clearly indicate purpose and scope.
*   **Defensive Programming**: Add defensive checks for unexpected data structures (as seen in the KeyError fixes).
*   **Comprehensive Logging**: Implement proper logging and debugging mechanisms for troubleshooting and monitoring.

### Testing and Validation
*   **Iterative Refinement**: The evolution from a two-pass to a three-pass system with external configuration demonstrates this principle.
*   **Quality Metrics**: Implement comprehensive quality metrics and validation at each stage of the pipeline.
*   **Automated Testing**: Use the `_apply.sh` script for automated end-to-end validation and regression testing.
*   **Edge Case Handling**: Test and handle edge cases (unusual PDF structures, oversized chunks, malformed content).

## Constraints & Preferences

### Programming Paradigm Adherence
*   **Functional Style**: Follow pure functional, explicit, and clean coding practices inspired by Bruce Eckel, Tom Gilb, and Bob Martin.
*   **Simplicity First**: Prioritize simplicity, clarity, modularity, and explicitness over clever or complex solutions.
*   **Code Smell Elimination**: Identify and remove code smells, such as duplicate code, long methods, and complex conditional statements.
*   **SOLID Principles**: Apply SOLID principles to enhance the overall design and maintainability of the code.

### Performance and Optimization
*   **Efficiency**: Optimize performance where applicable, considering time and space complexity.
*   **Memory Management**: Be mindful of memory usage, especially when processing large documents.
*   **Parallel Processing**: Use parallel processing for I/O-bound operations (LLM calls, file processing).
*   **Lazy Evaluation**: Use generator expressions and lazy evaluation where appropriate.

### Error Handling and Robustness
*   **Graceful Degradation**: Implement fallback strategies that allow the system to continue functioning when components fail.
*   **Comprehensive Validation**: Validate data at each stage of the pipeline with clear error messages.
*   **Defensive Programming**: Add defensive checks for unexpected inputs and edge cases.
*   **Logging Strategy**: Implement structured logging for debugging and monitoring without cluttering the output.

### Documentation and Maintainability
*   **Self-Documenting Code**: Write code that is self-explanatory through good naming and clear structure.
*   **Minimal Comments**: Lean towards no comments as much as possible. Include comments only when necessary for understanding complex logic.
*   **Configuration Documentation**: Provide clear examples and documentation for external configuration files.
*   **Testing Documentation**: Document testing procedures and validation criteria.

### External Dependencies and Configuration
*   **Minimal Dependencies**: Use well-established, maintained libraries with permissive licenses.
*   **External Configuration**: Prefer external configuration files (YAML) over hard-coded values for domain-specific settings.
*   **Environment Management**: Use environment variables for sensitive configuration (API keys).
*   **Dependency Management**: Maintain clean, conflict-free dependency specifications.

---

**For AI Agent Collaboration**: This enriched documentation provides complete workflow information, architectural details, configuration examples, validation procedures, and coding philosophy. The comprehensive testing infrastructure in `_apply.sh` provides immediate feedback on system health. The modular organization with clear separation of concerns (text cleaning, heading detection, extraction fallbacks, AI enrichment) follows Unix philosophy principles. The external tag configuration system allows domain-specific customization without code changes. Use the documented quality metrics, size limits, and validation procedures to maintain system reliability. The coding principles and constraints sections provide essential guidance for maintaining code quality and architectural consistency during future development.