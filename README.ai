# README.ai

## Project Intent

This project aims to construct a robust, reusable, standalone Python library specifically for chunking large PDF documents, preparing them for training and further interactions within a local LLM environment. The implementation emphasizes declarative and functional programming principles, with clear adherence to Unix philosophy—creating simple, focused modules communicating through structured interfaces.

## Project State

* Core PDF chunking logic and initial pipeline are defined and functional.
* Initial data preparation scripts exist and are functioning in prototype mode.

## Desired End-State

* Highly reliable and modular chunking library specialized for large PDF documents.
* Semantic-aware chunking capability integrated where beneficial (leveraging Haystack or RecursiveCharacterTextSplitter).
* Simple, clear CLI-based workflow facilitating rapid iteration and testing via shell scripts.

## Project Architecture

The library is organized into clearly defined modules and scripts:

```
pdf_chunker/
├── pdf_chunker/
│   ├── core.py          # Pure chunking logic and main pipeline definition
│   ├── parsing.py       # Extracting raw text from PDF documents
│   ├── splitter.py      # Semantic or structural chunking logic (Haystack or LangChain)
│   └── utils.py         # Pure utility functions (text cleaning, metadata handling)
├── scripts/
│   └── chunk_pdf.py     # Command-line interface for triggering chunking operations
├── tests/               # doesn't exist yet
│   └── test_core.py     # Unit tests validating core logic and integrations (doesn't exist yet)
├── pyproject.toml       # Project dependencies and build configuration (doesn't exist yet)
└── README.md            # Human-readable documentation and usage instructions (doesn't exist yet)
```

### File Purposes and Evolution

* **core.py**: Currently handles pipeline orchestration. This file will evolve to include more advanced pipeline control and improved functional composition.
* **parsing.py**: Handles PDF text extraction. Enhancements will focus on supporting various PDF complexities and formats.
* **splitter.py**: Implements chunking logic. Initially simple, will evolve towards sophisticated semantic-aware methods.
* **utils.py**: Manages common reusable functions. Future versions will incorporate improved metadata tagging and richer context management.
* **chunk\_pdf.py**: CLI script ensuring usability and integration with shell-based workflows. This will be extended to include more flexible user input handling and detailed logging.
* **test\_core.py**: Implements rigorous testing strategies to guarantee reliability. This will expand to cover all aspects of the pipeline comprehensively.

## Coding Principles & Style Guide

1. **Declarative & Functional Paradigm**

   * Implement pure functions wherever practical.
   * Clearly define inputs and outputs without side effects.

2. **Unix Philosophy**

   * "Do one thing and do it well." Keep modules small, focused, and composable.
   * Design modules to interact through simple textual or structured JSON outputs.

3. **Clear Interfaces & Data Flow**

   * Explicit input/output boundaries for predictable, testable behavior.
   * Avoid stateful or implicit behaviors; prefer explicit parameter passing.

4. **Testing**

   * Structure code to allow testing via simple shell scripts (`_apply.sh`).
   * Shell scripts should clearly indicate success, system state, or immediate problems.

5. **Iterative Refinement**

   * Provide a `_apply.sh` script at each checkpoint, validating functionality and correctness.
   * Shell scripts serve as self-documenting test and deployment aids.

6. **Metadata & Context Preservation**

   * Capture comprehensive metadata (document name, page number, section identifiers).
   * Metadata must clearly accompany data chunks for full traceability and debugging.

## Recommended Checkpoint Procedure

* At every iteration:

  1. Verify and validate outputs via the `_apply.sh` script.
  2. Confirm stable functional behavior after changes.
  3. Ensure seamless integration of new functionality with existing modules.

## Next Immediate Steps

1. Integrate and evaluate semantic-aware chunking (using Haystack).
2. Empirically validate semantic improvements through `_apply.sh` scripts.
3. Finalize metadata handling strategies for comprehensive downstream context capture.

## Constraints & Preferences

* Follow pure, explicit, and clean coding practices inspired by Bruce Eckel, Tom Gilb, and Bob Martin.
* Prioritize simplicity, clarity, modularity, and explicitness.

---

This document provides guidance for effectively achieving the outlined objectives through structured checkpoints, clear testing protocols, and disciplined coding practices.
