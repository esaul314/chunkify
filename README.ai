# README.ai

## Project Intent

This project aims to construct a robust, reusable, standalone Python library for chunking large **PDF and EPUB** documents, preparing them for training and further interactions within a local LLM environment. The implementation emphasizes declarative and functional programming principles, with clear adherence to Unix philosophy—creating simple, focused modules communicating through structured interfaces.

## Project State: Enhanced for RAG

The project has undergone a significant evolution to produce RAG-supportive, context-aware metadata.

> **⚠️ Known Issue:** There are edge cases relating to page numbering in multi-page PDFs with repeating content across pages, where chunk-to-source block mapping may assign incorrect page numbers. This can lead to chunks appearing in a non-intuitive order in the output JSONL. A potential solution would involve implementing a more sophisticated page-aware heuristic in the `_find_source_block` function in `utils.py`.
=======

*   **Three-Pass Architecture**: The pipeline now follows a **three-pass (Structural -> Semantic -> AI Enrichment) architecture**, adding a new layer of intelligent metadata generation.
*   **Modernized Core**: The entire processing backend has been upgraded to the modern **`haystack-ai`** framework, resolving critical dependency conflicts (`Pydantic V1 vs V2`) and ensuring future compatibility.
*   **Rich Metadata Generation**: The pipeline now generates a suite of RAG-supportive metadata for each chunk, including:
    *   **Language Detection**: Using `langdetect` on a per-block basis.
    *   **Readability Scores**: Calculated using `textstat` to provide Flesch-Kincaid grade levels.
    *   **AI-Powered Utterance Type**: A new, third pass uses an LLM (via `litellm`) to classify the semantic function of each chunk (e.g., `definition`, `instruction`, `example`).
*   **Robust Text Extraction**: EPUB parsing has been significantly improved to correctly handle inline formatting (`<i>`, `<b>`, etc.) and to strip invisible Unicode characters like the Byte Order Mark (`U+FEFF`), ensuring clean text for processing.
*   **Declarative & Functional Style**: The codebase has been refactored to more closely follow functional programming principles, using list comprehensions and generator expressions for data transformation where appropriate.

## Desired End-State

*   A highly reliable chunking library that understands the semantic structure of various document formats.
*   A clear, multi-stage pipeline that separates structural analysis from semantic chunking and AI enrichment.
*   A simple, clear CLI-based workflow facilitating rapid iteration and testing via shell scripts.

## Core Architecture: A Three-Pass Approach

The library is now designed around a three-pass philosophy to maximize the semantic integrity and usefulness of the final text chunks.

### 1. The Structural Pass
**Intention: Extract clean, structured text from the source document.**

This pass is handled by the `parsing.py` module. It analyzes the source file and converts it into an intermediate representation: a list of structured text blocks.

*   **PDF Parsing**: Uses **`PyMuPDF` (`fitz`)** for layout-aware parsing.
*   **EPUB Parsing**: Uses `EbookLib` and `BeautifulSoup`. The text extraction logic has been rewritten using a functional approach (`_get_element_text_content`) to correctly process text containing inline formatting tags (`<i>`, `<em>`, etc.) without introducing erroneous newlines or separators.
*   **Text Cleaning**: A low-level cleaning function (`_clean_paragraph`) now strips the Unicode BOM (`\uFEFF`) character and consolidates all whitespace, ensuring data purity from the very start.
*   **Language Detection**: `langdetect` is applied to each text block to determine its language.

### 2. The Semantic Pass
**Intention: Refine the structurally extracted text into semantically cohesive chunks.**

This pass is handled by the `splitter.py` module. It takes the text from all structured blocks, combines it, and splits it into appropriately sized chunks.

*   **Technology Upgrade**: The chunker was upgraded from the legacy `farm-haystack` to the modern **`haystack-ai`**. This resolved a `Pydantic` version conflict and modernized the stack.
*   **Sentence-Aware Chunking**: The implementation now uses `haystack.components.preprocessors.DocumentSplitter`, configured with `split_by="word"` and `respect_sentence_boundary=True`. This is the core of the sentence-aware chunking, ensuring that chunks are never ended mid-sentence.

### 3. The AI Enrichment Pass
**Intention: Add a layer of semantic understanding to each chunk for advanced RAG applications.**

This pass is handled by `ai_enrichment.py` and is orchestrated by `utils.py`.

*   **Utterance Type Classification**: For each chunk, a call is made to an LLM (e.g., GPT-3.5-Turbo via `litellm`) to classify its function. The prompt asks the model to choose from a list of types like `definition`, `explanation`, `instruction`, `example`, etc. This provides powerful metadata for filtering and routing in a RAG system.
*   **Flexible LLM Provider**: The use of `litellm` provides a consistent interface to over 100 LLM providers, making it easy to switch models or use local instances. API keys are managed securely via a `.env` file.
*   **Parallel Processing**: To handle the latency of LLM calls efficiently, the enrichment process is parallelized using a `ThreadPoolExecutor` in the `format_chunks_with_metadata` function.

## Project Architecture

```
pdf_chunker/
├── .env               # Holds API keys for AI services
├── pdf_chunker/
│   ��── core.py          # Orchestrates the three-pass pipeline
│   ├── parsing.py       # Structural Pass: Extracts and cleans text
│   ├── splitter.py      # Semantic Pass: Chunks text using Haystack
│   ├── utils.py         # Maps metadata and orchestrates AI enrichment
│   └── ai_enrichment.py # AI Pass: Classifies chunk utterance type
├── scripts/
│   └── chunk_pdf.py     # Command-line interface for the pipeline
...
```

### File Purposes and Evolution

*   **core.py**: Orchestrates the pipeline, now with logic to initialize the LLM and control the AI enrichment pass.
*   **parsing.py**: Enhanced with robust, functional text extraction for EPUBs and more thorough text cleaning (`\uFEFF` removal).
*   **splitter.py**: Upgraded to use the modern `haystack-ai`'s `DocumentSplitter`, ensuring correct, sentence-aware chunking.
*   **utils.py**: Now orchestrates the AI enrichment pass, calling the classifier in parallel and mapping all rich metadata (`language`, `readability`, `utterance_type`) to the final chunks.
*   **ai_enrichment.py**: A new, self-contained module responsible for all LLM-related logic. It's designed to be easily swappable or upgradable.
*   **.env**: A new file for securely storing the `OPENAI_API_KEY`, loaded via `python-dotenv`.

## Usage and Testing

### Setup
1.  Install dependencies: `./pdf-env/bin/pip install -r requirements.txt`
2.  Create a `.env` file in the project root and add your API key: `OPENAI_API_KEY='your-key-here'`

### Command-Line Usage
```shell
# With AI-enriched metadata (default)
./pdf-env/bin/python -m scripts.chunk_pdf path/to/your/document.pdf > output.jsonl

# Without any metadata
./pdf-env/bin/python -m scripts.chunk_pdf --no-metadata path/to/your/document.pdf > output_text_only.jsonl
```

### Validation with `_apply.sh`
The `_apply.sh` script remains the primary tool for testing functionality.
-   **Test PDF**: `./_apply.sh pdf`
-   **Test EPUB**: `./_apply.sh epub`

## Coding Principles & Style Guide

1.  **Declarative & Functional Paradigm**: Pure functions are preferred. Side effects are isolated. Data transformation is achieved through comprehensions and generator expressions, as seen in the EPUB parsing logic in `parsing.py`.
2.  **Unix Philosophy**: "Do one thing and do it well." The new `ai_enrichment.py` module is a prime example, encapsulating all AI-related logic.
3.  **Clear Interfaces & Data Flow**: The three-pass architecture maintains a clear, structured data flow, passing data between well-defined stages.
4.  **Testing**: The `_apply.sh` script is used at each checkpoint to validate functionality and correctness.
5.  **Iterative Refinement**: The evolution from a two-pass to a three-pass system, and the upgrade of the core Haystack library, demonstrate this principle.
6.  **Metadata & Context Preservation**: The pipeline now excels at this, providing traceable and semantically rich metadata for each chunk, making the output highly valuable for RAG systems.

## Constraints & Preferences

*   Follow pure, explicit, and clean coding practices inspired by Bruce Eckel, Tom Gilb, and Bob Martin.
*   Prioritize simplicity, clarity, modularity, and explicitness.

---

This document provides guidance for effectively achieving the outlined objectives through structured checkpoints, clear testing protocols, and disciplined coding practices.
